{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPModel(\n",
      "  (model): CLIP(\n",
      "    (visual): VisionTransformer(\n",
      "      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (transformer): Transformer(\n",
      "        (resblocks): Sequential(\n",
      "          (0): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (1): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (3): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (4): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (5): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (6): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (7): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (8): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (9): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (10): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (11): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (12): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (13): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (14): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (15): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (16): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (17): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (18): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (19): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (20): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (21): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (22): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (23): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (resblocks): Sequential(\n",
      "        (0): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (token_embedding): Embedding(49408, 768)\n",
      "    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models import get_model\n",
    "model = get_model('CLIP:ViT-L/14@336px')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
